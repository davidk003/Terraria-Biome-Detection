{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d2ccaa24",
      "metadata": {},
      "source": [
        "Press q in opencv window to stop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "Capture backend: mss\n",
            "Model path: C:\\Users\\David Kim\\Documents\\Programming\\Terraria-Biome-Detection\\checkpoints\\final_efficientnet_v2_s.pth\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import mss\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "MODEL_PATH = Path('checkpoints/final_efficientnet_v2_s.pth')\n",
        "if not MODEL_PATH.exists():\n",
        "    MODEL_PATH = Path('../checkpoints/final_efficientnet_v2_s.pth')\n",
        "if not MODEL_PATH.exists():\n",
        "    raise FileNotFoundError(f'Model file not found: {MODEL_PATH.resolve()}')\n",
        "\n",
        "IMG_SIZE = 224\n",
        "NUM_CLASSES = 10\n",
        "MEAN = [0.4914, 0.4822, 0.4465]\n",
        "STD = [0.2470, 0.2435, 0.2616]\n",
        "CLASS_NAMES = [\n",
        "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "    'dog', 'frog', 'horse', 'ship', 'truck',\n",
        "]\n",
        "\n",
        "CAPTURE_BACKEND = 'mss'  # 'mss' or 'opencv_camera'\n",
        "USE_PRIMARY_MONITOR = True\n",
        "MONITOR_INDEX = 1\n",
        "CAPTURE_REGION = {'top': 120, 'left': 200, 'width': 1280, 'height': 720}\n",
        "CAMERA_INDEX = 0\n",
        "\n",
        "TARGET_FPS = 10\n",
        "WINDOW_NAME = 'Terraria Biome Detection'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if CAPTURE_BACKEND not in {'mss', 'opencv_camera'}:\n",
        "    raise ValueError(\"CAPTURE_BACKEND must be 'mss' or 'opencv_camera'.\")\n",
        "\n",
        "print('Device:', DEVICE)\n",
        "print('Capture backend:', CAPTURE_BACKEND)\n",
        "print('Model path:', MODEL_PATH.resolve())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model():\n",
        "    model = torchvision.models.efficientnet_v2_s(weights=None, num_classes=NUM_CLASSES)\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    return model.to(DEVICE).eval()\n",
        "\n",
        "\n",
        "def preprocess(frame_bgr):\n",
        "    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "    rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "    x = torch.from_numpy(rgb).permute(2, 0, 1).float().div_(255.0)\n",
        "    x = TF.normalize(x, MEAN, STD)\n",
        "    return x.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "\n",
        "def predict(frame_bgr):\n",
        "    with torch.inference_mode():\n",
        "        probs = torch.softmax(MODEL(preprocess(frame_bgr)), dim=1)[0]\n",
        "    idx = int(probs.argmax().item())\n",
        "    return CLASS_NAMES[idx], float(probs[idx].item())\n",
        "\n",
        "\n",
        "def draw_overlay(frame, label, conf, fps):\n",
        "    out = frame.copy()\n",
        "    cv2.putText(out, f'Pred: {label}', (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)\n",
        "    cv2.putText(out, f'Conf: {conf:.3f}', (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 255), 2)\n",
        "    cv2.putText(out, f'FPS: {fps:.1f}', (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 200, 0), 2)\n",
        "    cv2.putText(out, 'Press q to quit', (20, out.shape[0] - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 200), 2)\n",
        "    return out\n",
        "\n",
        "\n",
        "MODEL = load_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Capture region: {'left': 0, 'top': 0, 'width': 1920, 'height': 1080}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     29\u001b[39m     t0 = time.perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     frame = \u001b[43mgrab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     label, conf = predict(frame)\n\u001b[32m     33\u001b[39m     dt = \u001b[38;5;28mmax\u001b[39m(time.perf_counter() - t0, \u001b[32m1e-6\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mgrab\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgrab\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.asarray(\u001b[43msct\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m)\u001b[49m, dtype=np.uint8)[:, :, :\u001b[32m3\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\David Kim\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\mss\\base.py:101\u001b[39m, in \u001b[36mMSSBase.grab\u001b[39m\u001b[34m(self, monitor)\u001b[39m\n\u001b[32m     93\u001b[39m     monitor = {\n\u001b[32m     94\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m: monitor[\u001b[32m0\u001b[39m],\n\u001b[32m     95\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtop\u001b[39m\u001b[33m\"\u001b[39m: monitor[\u001b[32m1\u001b[39m],\n\u001b[32m     96\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwidth\u001b[39m\u001b[33m\"\u001b[39m: monitor[\u001b[32m2\u001b[39m] - monitor[\u001b[32m0\u001b[39m],\n\u001b[32m     97\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mheight\u001b[39m\u001b[33m\"\u001b[39m: monitor[\u001b[32m3\u001b[39m] - monitor[\u001b[32m1\u001b[39m],\n\u001b[32m     98\u001b[39m     }\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     screenshot = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grab_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_cursor \u001b[38;5;129;01mand\u001b[39;00m (cursor := \u001b[38;5;28mself\u001b[39m._cursor_impl()):\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._merge(screenshot, cursor)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\David Kim\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\mss\\windows.py:240\u001b[39m, in \u001b[36mMSS._grab_impl\u001b[39m\u001b[34m(self, monitor)\u001b[39m\n\u001b[32m    237\u001b[39m     \u001b[38;5;28mself\u001b[39m._handles.bmp = gdi.CreateCompatibleBitmap(srcdc, width, height)\n\u001b[32m    238\u001b[39m     gdi.SelectObject(memdc, \u001b[38;5;28mself\u001b[39m._handles.bmp)\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[43mgdi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBitBlt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemdc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrcdc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSRCCOPY\u001b[49m\u001b[43m \u001b[49m\u001b[43m|\u001b[49m\u001b[43m \u001b[49m\u001b[43mCAPTUREBLT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m bits = gdi.GetDIBits(memdc, \u001b[38;5;28mself\u001b[39m._handles.bmp, \u001b[32m0\u001b[39m, height, \u001b[38;5;28mself\u001b[39m._handles.data, \u001b[38;5;28mself\u001b[39m._handles.bmi, DIB_RGB_COLORS)\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bits != height:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "if CAPTURE_BACKEND == 'mss':\n",
        "    sct = mss.mss()\n",
        "    monitor = sct.monitors[MONITOR_INDEX] if USE_PRIMARY_MONITOR else CAPTURE_REGION\n",
        "    print('Capture region:', monitor)\n",
        "\n",
        "    def grab():\n",
        "        return np.asarray(sct.grab(monitor), dtype=np.uint8)[:, :, :3]\n",
        "\n",
        "    cleanup = sct.close\n",
        "else:\n",
        "    cap = cv2.VideoCapture(CAMERA_INDEX)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f'Could not open camera index {CAMERA_INDEX}.')\n",
        "    print(f'Using camera backend at index {CAMERA_INDEX}')\n",
        "\n",
        "    def grab():\n",
        "        ok, frame = cap.read()\n",
        "        if not ok or frame is None:\n",
        "            raise RuntimeError('Failed to read frame from camera backend.')\n",
        "        return frame\n",
        "\n",
        "    cleanup = cap.release\n",
        "\n",
        "frame_interval = 1.0 / max(1, TARGET_FPS)\n",
        "cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_NORMAL)\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        t0 = time.perf_counter()\n",
        "        frame = grab()\n",
        "        label, conf = predict(frame)\n",
        "\n",
        "        dt = max(time.perf_counter() - t0, 1e-6)\n",
        "        fps_display = 1.0 / dt\n",
        "\n",
        "        cv2.imshow(WINDOW_NAME, draw_overlay(frame, label, conf, fps_display))\n",
        "        if (cv2.waitKey(1) & 0xFF) == ord('q'):\n",
        "            break\n",
        "\n",
        "        sleep_time = frame_interval - dt\n",
        "        if sleep_time > 0:\n",
        "            time.sleep(sleep_time)\n",
        "finally:\n",
        "    cleanup()\n",
        "    cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
